êµ¬ì„± ìš”ì†Œ	ì„¤ëª…

ê¸°ëŠ¥	ìƒíƒœ
âœ… ì–¼êµ´ í‘œì • ì¸ì‹ (ê¸°ì¨, í™”ë‚¨ ë“±)	
âœ… Google STTë¡œ ì‚¬ìš©ìì˜ ë§ ì¸ì‹	
âœ… ê°ì • + ë§ â†’ ê³µê°í˜• ë¬¸ì¥ ìƒì„±	
âœ… Edge-TTSë¡œ ê°ì •ì´ ë‹´ê¸´ ìŒì„± ì¶œë ¥	
âœ… ì¤‘ë³µ ì‘ë‹µ ë°©ì§€ / ì—ëŸ¬ ì²˜ë¦¬	
âœ… "ì¢…ë£Œ", "ê·¸ë§Œ" ë§í•˜ë©´ ì¢…ë£Œ	
âœ… ìŒì„± ì¬ìƒ í›„ mp3 ìë™ ì‚­ì œ
--------------------------------------------------------

íŒŒì¼ëª… : face_voice_emotion_ai.py

import tensorflow.keras
import numpy as np
import cv2
import speech_recognition as sr
import asyncio
import edge_tts
import time
import playsound
import os

# ----- ê°ì • ë¼ë²¨ ë¶ˆëŸ¬ì˜¤ê¸° -----
with open("labels.txt", "r", encoding="utf-8") as f:
    classes = [line.strip().split(' ')[1] for line in f.readlines()]

# ----- ê°ì • â†’ ë¬¸ì¥ + ìŠ¤íƒ€ì¼ ë§¤í•‘ -----
def generate_response(emotion, user_text):
    if emotion == "ê¸°ì¨":
        return f"ì›ƒëŠ” ëª¨ìŠµì´ ë³´ê¸° ì¢‹ì•„ìš”! '{user_text}' ë¼ê³  ë§ì”€í•˜ì…¨êµ°ìš”.", "cheerful"
    elif emotion == "í™”ë‚¨":
        return f"ì¡°ê¸ˆ ë¶ˆí¸í•˜ì‹  ê²ƒ ê°™ë„¤ìš”. '{user_text}' ë¼ê³  í•˜ì…¨ì£ ?", "angry"
    elif emotion == "ì–µìš¸":
        return f"ì–µìš¸í•œ ì¼ì´ ìˆì—ˆêµ°ìš”. '{user_text}' ê¸°ì–µí• ê²Œìš”.", "sad"
    elif emotion == "ë¬´í‘œì •":
        return f"ì¡°ìš©í•œ ë¶„ìœ„ê¸°ë„¤ìš”. '{user_text}' ë¼ê³  ë§ì”€í•˜ì…¨ì–´ìš”.", "calm"
    else:
        return f"'{user_text}' ë¼ê³  ë§ì”€í•˜ì…¨ì–´ìš”.", "default"

# ----- ê°ì • ìŒì„± ì¶œë ¥ -----
async def speak(text, style, voice="ko-KR-SunHiNeural"):
    ssml = f"""
    <speak version='1.0' xml:lang='ko-KR'>
      <voice name='{voice}'>
        <express-as style='{style}'>{text}</express-as>
      </voice>
    </speak>
    """
    communicate = edge_tts.Communicate(ssml, voice=voice)
    await communicate.save("voice.mp3")
    try:
        playsound.playsound("voice.mp3")
    except Exception as e:
        print("ìŒì„± ì¬ìƒ ì˜¤ë¥˜:", e)
    finally:
        if os.path.exists("voice.mp3"):
            os.remove("voice.mp3")

# ----- í‘œì • ë¶„ì„ -----
def detect_emotion_from_webcam(model):
    cap = cv2.VideoCapture(0)
    size = (224, 224)
    ret, img = cap.read()
    cap.release()
    if not ret:
        return None
    img = img[:, 100:100+img.shape[0]]
    img = cv2.flip(img, 1)
    img_input = cv2.resize(img, size)
    img_input = cv2.cvtColor(img_input, cv2.COLOR_BGR2RGB)
    img_input = (img_input.astype(np.float32) / 127.0) - 1
    img_input = np.expand_dims(img_input, axis=0)
    prediction = model.predict(img_input)
    idx = np.argmax(prediction)
    return classes[idx]

# ----- ìŒì„± ì…ë ¥ (Google STT) -----
def get_user_speech():
    recognizer = sr.Recognizer()
    mic = sr.Microphone()
    with mic as source:
        print("ğŸ¤ ë§ì„ í•´ë³´ì„¸ìš”...")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)
    try:
        print("ğŸ“ ìŒì„± í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")
        return recognizer.recognize_google(audio, language='ko-KR')
    except Exception as e:
        print("ìŒì„± ì¸ì‹ ì‹¤íŒ¨:", e)
        return ""

# ----- ë©”ì¸ ì‹¤í–‰ -----
if __name__ == "__main__":
    print("ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = tensorflow.keras.models.load_model("keras_model.h5")

    while True:
        print("\nâ–¶ í‘œì • ë° ìŒì„± ì…ë ¥ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ' ë˜ëŠ” 'ê·¸ë§Œ' ë§í•˜ê¸°)")
        emotion = detect_emotion_from_webcam(model)
        if not emotion:
            print("âŒ ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨. ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
            continue
        print("ğŸ˜Š ì¸ì‹ëœ ê°ì •:", emotion)

        user_text = get_user_speech()
        if not user_text.strip():
            print("âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨. ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
            continue

        print("ğŸ—£ï¸ ì¸ì‹ëœ ë§:", user_text)

        if "ì¢…ë£Œ" in user_text or "ê·¸ë§Œ" in user_text:
            print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        reply_text, style = generate_response(emotion, user_text)
        print(f"ğŸ“¢ AI ì‘ë‹µ: {reply_text} ({style})")
        asyncio.run(speak(reply_text, style))
