êµ¬ì„± ìš”ì†Œ	ì„¤ëª…

ê°ì • ì¸ì‹ (ì¹´ë©”ë¼)   >>  ì–¼êµ´ í‘œì •ìœ¼ë¡œ ê°ì • íŒë‹¨              
Google STT API >>  **ì‚¬ëŒì˜ ë§ì„ ë¬¸ìë¡œ ë³€í™˜**                  
Edge TTS  >> ê°ì • ìŠ¤íƒ€ì¼ì„ ë‹´ì€ ìŒì„± ìƒì„±                
ìµœì¢… ì¶œë ¥  >> "í™”ê°€ ë‚˜ì‹  ê²ƒ ê°™ì•„ìš”. 'ë¬´ìŠ¨ ì¼ì´ì•¼!' ë¼ê³  ë§ì”€í•˜ì…¨ì–´ìš”." ë“±ë“±

--------------------------------------------------------
Code
  
import tensorflow.keras
import numpy as np
import cv2
import speech_recognition as sr
import asyncio
import edge_tts
import time
import playsound

# ----- ê°ì • ë¼ë²¨ ë¶ˆëŸ¬ì˜¤ê¸° -----
with open("labels.txt", "r", encoding="utf-8") as f:
    classes = [line.strip().split(' ')[1] for line in f.readlines()]

# ----- ê°ì • â†’ ë¬¸ì¥ + ìŠ¤íƒ€ì¼ ë§¤í•‘ -----
def generate_response(emotion, user_text):
    if emotion == "ê¸°ì¨":
        return f"ì›ƒëŠ” ëª¨ìŠµì´ ë³´ê¸° ì¢‹ì•„ìš”! '{user_text}' ë¼ê³  ë§ì”€í•˜ì…¨êµ°ìš”.", "cheerful"
    elif emotion == "í™”ë‚¨":
        return f"ì¡°ê¸ˆ ë¶ˆí¸í•˜ì‹  ê²ƒ ê°™ë„¤ìš”. '{user_text}' ë¼ê³  í•˜ì…¨ì£ ?", "angry"
    elif emotion == "ì–µìš¸":
        return f"ì–µìš¸í•œ ì¼ì´ ìˆì—ˆêµ°ìš”. '{user_text}' ê¸°ì–µí• ê²Œìš”.", "sad"
    elif emotion == "ë¬´í‘œì •":
        return f"ì¡°ìš©í•œ ë¶„ìœ„ê¸°ë„¤ìš”. '{user_text}' ë¼ê³  ë§ì”€í•˜ì…¨ì–´ìš”.", "calm"
    else:
        return f"'{user_text}' ë¼ê³  ë§ì”€í•˜ì…¨ì–´ìš”.", "default"

# ----- ê°ì • ìŒì„± ì¶œë ¥ -----
async def speak(text, style, voice="ko-KR-SunHiNeural"):
    ssml = f"""
    <speak version='1.0' xml:lang='ko-KR'>
      <voice name='{voice}'>
        <express-as style='{style}'>{text}</express-as>
      </voice>
    </speak>
    """
    communicate = edge_tts.Communicate(ssml, voice=voice)
    await communicate.save("voice.mp3")
    playsound.playsound("voice.mp3")

# ----- í‘œì • ë¶„ì„ -----
def detect_emotion_from_webcam(model):
    cap = cv2.VideoCapture(0)
    size = (224, 224)
    ret, img = cap.read()
    cap.release()
    if not ret:
        return None
    img = img[:, 100:100+img.shape[0]]
    img = cv2.flip(img, 1)
    img_input = cv2.resize(img, size)
    img_input = cv2.cvtColor(img_input, cv2.COLOR_BGR2RGB)
    img_input = (img_input.astype(np.float32) / 127.0) - 1
    img_input = np.expand_dims(img_input, axis=0)
    prediction = model.predict(img_input)
    idx = np.argmax(prediction)
    return classes[idx]

# ----- ìŒì„± ì…ë ¥ (Google STT) -----
def get_user_speech():
    recognizer = sr.Recognizer()
    mic = sr.Microphone()
    with mic as source:
        print("ğŸ¤ ë§ì„ í•´ë³´ì„¸ìš”...")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)
    try:
        print("ğŸ“ ìŒì„± í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")
        return recognizer.recognize_google(audio, language='ko-KR')
    except Exception as e:
        print("ìŒì„± ì¸ì‹ ì‹¤íŒ¨:", e)
        return ""

# ----- ë©”ì¸ ì‹¤í–‰ -----
if __name__ == "__main__":
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = tensorflow.keras.models.load_model("keras_model.h5")

    while True:
        print("\nâ–¶ í‘œì • ë° ìŒì„± ì…ë ¥ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+C)")
        emotion = detect_emotion_from_webcam(model)
        print("ğŸ˜Š ì¸ì‹ëœ í‘œì • ê°ì •:", emotion)

        user_text = get_user_speech()
        print("ğŸ—£ï¸ ì¸ì‹ëœ ë§:", user_text)

        if emotion and user_text:
            reply_text, style = generate_response(emotion, user_text)
            print(f"ğŸ“¢ ì‘ë‹µ: {reply_text} ({style})")
            asyncio.run(speak(reply_text, style))
