구성 요소	설명

기능	상태
✅ 얼굴 표정 인식 (기쁨, 화남 등)	
✅ Google STT로 사용자의 말 인식	
✅ 감정 + 말 → 공감형 문장 생성	
✅ Edge-TTS로 감정이 담긴 음성 출력	
✅ 중복 응답 방지 / 에러 처리	
✅ "종료", "그만" 말하면 종료	
✅ 음성 재생 후 mp3 자동 삭제
--------------------------------------------------------

파일명 : face_voice_emotion_ai.py

import tensorflow.keras
import numpy as np
import cv2
import speech_recognition as sr
import asyncio
import edge_tts
import time
import playsound
import os

# ----- 감정 라벨 불러오기 -----
with open("labels.txt", "r", encoding="utf-8") as f:
    classes = [line.strip().split(' ')[1] for line in f.readlines()]

# ----- 감정 → 문장 + 스타일 매핑 -----
def generate_response(emotion, user_text):
    if emotion == "기쁨":
        return f"웃는 모습이 보기 좋아요! '{user_text}' 라고 말씀하셨군요.", "cheerful"
    elif emotion == "화남":
        return f"조금 불편하신 것 같네요. '{user_text}' 라고 하셨죠?", "angry"
    elif emotion == "억울":
        return f"억울한 일이 있었군요. '{user_text}' 기억할게요.", "sad"
    elif emotion == "무표정":
        return f"조용한 분위기네요. '{user_text}' 라고 말씀하셨어요.", "calm"
    else:
        return f"'{user_text}' 라고 말씀하셨어요.", "default"

# ----- 감정 음성 출력 -----
async def speak(text, style, voice="ko-KR-SunHiNeural"):
    ssml = f"""
    <speak version='1.0' xml:lang='ko-KR'>
      <voice name='{voice}'>
        <express-as style='{style}'>{text}</express-as>
      </voice>
    </speak>
    """
    communicate = edge_tts.Communicate(ssml, voice=voice)
    await communicate.save("voice.mp3")
    try:
        playsound.playsound("voice.mp3")
    except Exception as e:
        print("음성 재생 오류:", e)
    finally:
        if os.path.exists("voice.mp3"):
            os.remove("voice.mp3")

# ----- 표정 분석 -----
def detect_emotion_from_webcam(model):
    cap = cv2.VideoCapture(0)
    size = (224, 224)
    ret, img = cap.read()
    cap.release()
    if not ret:
        return None
    img = img[:, 100:100+img.shape[0]]
    img = cv2.flip(img, 1)
    img_input = cv2.resize(img, size)
    img_input = cv2.cvtColor(img_input, cv2.COLOR_BGR2RGB)
    img_input = (img_input.astype(np.float32) / 127.0) - 1
    img_input = np.expand_dims(img_input, axis=0)
    prediction = model.predict(img_input)
    idx = np.argmax(prediction)
    return classes[idx]

# ----- 음성 입력 (Google STT) -----
def get_user_speech():
    recognizer = sr.Recognizer()
    mic = sr.Microphone()
    with mic as source:
        print("🎤 말을 해보세요...")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)
    try:
        print("📝 음성 텍스트로 변환 중...")
        return recognizer.recognize_google(audio, language='ko-KR')
    except Exception as e:
        print("음성 인식 실패:", e)
        return ""

# ----- 메인 실행 -----
if __name__ == "__main__":
    print("🤖 모델 로딩 중...")
    model = tensorflow.keras.models.load_model("keras_model.h5")

    while True:
        print("\n▶ 표정 및 음성 입력을 시작합니다. (종료하려면 '종료' 또는 '그만' 말하기)")
        emotion = detect_emotion_from_webcam(model)
        if not emotion:
            print("❌ 얼굴 인식 실패. 다시 시도합니다.")
            continue
        print("😊 인식된 감정:", emotion)

        user_text = get_user_speech()
        if not user_text.strip():
            print("❌ 음성 인식 실패. 다시 시도합니다.")
            continue

        print("🗣️ 인식된 말:", user_text)

        if "종료" in user_text or "그만" in user_text:
            print("👋 대화를 종료합니다.")
            break

        reply_text, style = generate_response(emotion, user_text)
        print(f"📢 AI 응답: {reply_text} ({style})")
        asyncio.run(speak(reply_text, style))
