led_yellow: GPIO 17번 핀 (happy)
led_red: GPIO 4번 핀 (expressionless)
led_green: GPIO 27번 핀 (surprised)


import tensorflow.keras
import numpy as np
import cv2
import pygame
import time
from gpiozero import LED

# TensorFlow 모델 로드
model = tensorflow.keras.models.load_model('/home/pi/cos/converted_keras/converted_keras/keras_model.h5')

# OpenCV로 웹캡 캡처 시작
cap = cv2.VideoCapture(0)
size = (224, 224)
classes = ['happy', 'expressionless', 'surprised']

# Pygame 초기화 및 음악 파일 로드
pygame.mixer.init()
music_files = {
    'happy': '/home/pi/cos/converted_keras/happy.mp3',
    'expressionless': '/home/pi/cos/converted_keras/expressionless.mp3',
    'surprised': '/home/pi/cos/converted_keras/surprised.mp3'
}

# LED 설정 (GPIO 핀 번호)
led_yellow = LED(17)  # happy
led_red = LED(4)      # expressionless
led_green = LED(27)   # surprised

# 예측 확률 임계값 설정
THRESHOLD = 0.6

def play_music(emotion):
    # 감정이 None이 아닐 때만 음악 재생
    if emotion == 'happy':
        led_yellow.on()
        led_red.off()
        led_green.off()
        pygame.mixer.music.load(music_files[emotion])
        pygame.mixer.music.play()
        time.sleep(60)
        pygame.mixer.music.stop()
    elif emotion == 'expressionless':
        led_yellow.off()
        led_red.on()
        led_green.off()
        pygame.mixer.music.load(music_files[emotion])
        pygame.mixer.music.play()
        time.sleep(60)
        pygame.mixer.music.stop()
    elif emotion == 'surprised':
        led_yellow.off()
        led_red.off()
        led_green.on()
        pygame.mixer.music.load(music_files[emotion])
        pygame.mixer.music.play()
        time.sleep(60)
        pygame.mixer.music.stop()

    # 모든 LED 끄기
    led_yellow.off()
    led_red.off()
    led_green.off()

# 웹캠에서 이미지 인식 및 음악 재생 루프
while cap.isOpened():
    ret, img = cap.read()
    if not ret:
        break

    h, w, _ = img.shape
    cx = h / 2
    img = img[:, 100:100 + img.shape[0]]
    img = cv2.flip(img, 1)

    # 이미지 전처리
    img_input = cv2.resize(img, size)
    img_input = cv2.cvtColor(img_input, cv2.COLOR_BGR2RGB)
    img_input = (img_input.astype(np.float32) / 127.0) - 1
    img_input = np.expand_dims(img_input, axis=0)

    # 모델 예측
    prediction = model.predict(img_input)
    idx = np.argmax(prediction)
    max_probability = prediction[0][idx]  # 가장 높은 확률 값
    detected_emotion = classes[idx] if max_probability >= THRESHOLD else "None"

    # 결과 표시
    smaller_img = cv2.resize(img, (800, 600))
    cv2.putText(smaller_img, text=detected_emotion, org=(10, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX,
                fontScale=0.8, color=(255, 255, 255), thickness=2)
    cv2.imshow('result', smaller_img)

    # 인식된 표정에 따라 음악 재생 및 LED 제어
    if detected_emotion != "None":
        play_music(detected_emotion)

    # 'q' 키를 누르면 종료
    if cv2.waitKey(1) == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()



임계값을 설정해서 확률이 낮을 경우 "None"으로 감지되도록 하고, 
그 상태에서는 음악이 재생되지 않게끔 코드를 수정하면 됩니다.

아래는 수정된 코드입니다. 임계값(THRESHOLD)을 설정해 인식된 감정의 확률이 그보다 낮으면 음악이 재생되지 않도록 합니다:
THRESHOLD 설정: 모델이 예측한 결과의 확률이 THRESHOLD 이상일 때만 감정을 "감지된" 것으로 처리합니다.
play_music 함수 호출 조건 변경: detected_emotion이 "None"이 아닐 때만 play_music 함수를 호출하게 변경하였습니다. 
이렇게 하면 확률이 낮아 감정이 인식되지 않을 때는 음악이 재생되지 않습니다.
감정 인식 실패 시 LED 및 음악 재생 방지: 감정 인식이 되지 않을 때는 모든 LED를 끄고 음악이 재생되지 않도록 구현했습니다.
